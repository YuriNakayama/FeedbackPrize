{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72bf1e6f",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cda5c0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:15.974474Z",
     "start_time": "2022-08-16T09:43:13.924070Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from ignite.handlers import create_lr_scheduler_with_warmup\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import (StratifiedGroupKFold, StratifiedKFold,\n",
    "                                     train_test_split)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer,\n",
    "                          DataCollatorWithPadding)\n",
    "\n",
    "#import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38091ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:15.980184Z",
     "start_time": "2022-08-16T09:43:15.977192Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_column\", 100)\n",
    "pd.set_option(\"display.max_row\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471a1df",
   "metadata": {},
   "source": [
    "# パラメータの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b446a55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:16.009601Z",
     "start_time": "2022-08-16T09:43:15.982018Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# パラメータの設定\n",
    "MODELNAME=\"microsoft/deberta-v3-base\"\n",
    "config = AutoConfig.from_pretrained(MODELNAME).to_dict()\n",
    "config[\"model_name\"] = MODELNAME\n",
    "config[\"max_token_len\"] = 512\n",
    "config[\"drop_rate\"] = 0.4\n",
    "config[\"output_size\"] = 3\n",
    "config[\"fold_split\"] = 3\n",
    "\n",
    "config[\"train_batch_size\"] = 16\n",
    "config[\"valid_batch_size\"] = 16\n",
    "config[\"num_epochs\"] = 4\n",
    "\n",
    "config[\"learning_rate\"] = 1e-5\n",
    "config[\"lr_T_max\"] = 500\n",
    "config[\"min_lr\"] = 1e-6\n",
    "config[\"weight_decay\"] = 0.005\n",
    "# config[\"warmup_start_value\"] = 0.0\n",
    "# config[\"warmup_end_value\"] = 0.1\n",
    "# config[\"warmup_duration\"] = 3\n",
    "\n",
    "config[\"label_smoothing\"] = 0.2\n",
    "\n",
    "config[\"gradient_checkpoint\"] = True\n",
    "config[\"freezing\"] = True\n",
    "config[\"header_type\"] = \"Concatenate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9f556",
   "metadata": {},
   "source": [
    "# データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2492583a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:16.014737Z",
     "start_time": "2022-08-16T09:43:16.011278Z"
    }
   },
   "outputs": [],
   "source": [
    "essay_data_path = \"/home/jovyan/work/data/train/\"\n",
    "def get_essay(essay_id):\n",
    "    essay_path = os.path.join(essay_data_path, f\"{essay_id}.txt\")\n",
    "    essay_text = open(essay_path, 'r').read()\n",
    "    return essay_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640c2196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:16.150634Z",
     "start_time": "2022-08-16T09:43:16.017016Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/jovyan/work/data/train.csv\",\n",
    "    index_col=\"discourse_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c143fac9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:16.992467Z",
     "start_time": "2022-08-16T09:43:16.152362Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"essay_text\"] = df[\"essay_id\"].apply(get_essay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e5752",
   "metadata": {},
   "source": [
    "## データのラベル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d318091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:16.997117Z",
     "start_time": "2022-08-16T09:43:16.994185Z"
    }
   },
   "outputs": [],
   "source": [
    "discourse_types = [\"Lead\", \"Position\", \"Claim\", \"Evidence\", \"Counterclaim\", \"Concluding Statement\", \"Rebuttal\"]\n",
    "dicourse_effectiveness_cols = [\"Ineffective\", \"Effective\", \"Adequate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33623ba2",
   "metadata": {},
   "source": [
    "## データ整形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14985949",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:18.148573Z",
     "start_time": "2022-08-16T09:43:16.998849Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#下処理:discourse_typeとdiscourse_textとessayを結合する\n",
    "sep = AutoTokenizer.from_pretrained(config[\"model_name\"]).sep_token\n",
    "df[\"inputs\"] = df.discourse_type + sep + df.discourse_text + sep + df.essay_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfaac2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:18.164682Z",
     "start_time": "2022-08-16T09:43:18.151047Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 目的変数をone hotエンコーディングする\n",
    "df = pd.concat([df, pd.get_dummies(df.discourse_effectiveness)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b2235a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:18.205472Z",
     "start_time": "2022-08-16T09:43:18.166913Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# データの分割\n",
    "df_train_valid, df_test = train_test_split(\n",
    "    df, test_size=0.2, shuffle=True, random_state=0, stratify=df[\"discourse_effectiveness\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92a5a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:18.213766Z",
     "start_time": "2022-08-16T09:43:18.207430Z"
    }
   },
   "outputs": [],
   "source": [
    "# indexを整数連番に\n",
    "df_train_valid = df_train_valid.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f238400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.038793Z",
     "start_time": "2022-08-16T09:43:18.215802Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stratified K foldの作成\n",
    "sgkf = StratifiedGroupKFold(n_splits=config[\"fold_split\"], random_state=None)\n",
    "\n",
    "for fold, (_train_index, _valid_index) in enumerate(\n",
    "    sgkf.split(\n",
    "        X=df_train_valid,\n",
    "        y=df_train_valid.discourse_effectiveness,\n",
    "        groups=df_train_valid.essay_id,\n",
    "    )\n",
    "):\n",
    "    df_train_valid.loc[_valid_index, \"kfold\"] = int(fold)\n",
    "    \n",
    "df_train_valid[\"kfold\"] = df_train_valid[\"kfold\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c041a",
   "metadata": {},
   "source": [
    "# definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649eebaf",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ed5c9f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.044632Z",
     "start_time": "2022-08-16T09:43:19.040569Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "#textをtokenizeするクラス(前処理)\n",
    "class tokenizer(object):\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.padding_side=tokenizer.padding_side\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __call__(self, text):\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b04cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.055383Z",
     "start_time": "2022-08-16T09:43:19.048104Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Dynamic Padding (Collate)\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "        # self.args = args\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        output[\"token_type_ids\"] = [sample[\"token_type_ids\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"labels\"] = [sample[\"labels\"].tolist() for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "            output[\"token_type_ids\"] = [s + (batch_max - len(s)) * [0] for s in output[\"token_type_ids\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "            output[\"token_type_ids\"] = [(batch_max - len(s)) * [0] + s for s in output[\"token_type_ids\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        output[\"token_type_ids\"] = torch.tensor(output[\"token_type_ids\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"labels\"] = torch.tensor(output[\"labels\"], dtype=torch.float)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "806f1a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.063019Z",
     "start_time": "2022-08-16T09:43:19.057127Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Datasetの定義\n",
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, X, y, transform):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform=transform\n",
    "    def __len__(self):  # len(Dataset)で返す値を指定\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):  # Dataset[index]で返す値を指定\n",
    "        text = self.X[index]\n",
    "        output = self.transform(text)\n",
    "        output[\"labels\"] = self.y[index]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af2a2e29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.070834Z",
     "start_time": "2022-08-16T09:43:19.064729Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Subsetの定義\n",
    "class CreateSubset(Dataset):\n",
    "    def __init__(self, X, y, transform, indices):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.indices = indices\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):  # len(Dataset)で返す値を指定\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):  # Dataset[index]で返す値を指定\n",
    "        _X = self.X[self.indices[idx]]\n",
    "        output_dict = self.transform(_X)\n",
    "        output_dict[\"labels\"] = self.y[self.indices[idx]]\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56304f5",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ebe9e11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.076202Z",
     "start_time": "2022-08-16T09:43:19.072361Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# freezing\n",
    "def freeze(module):\n",
    "    for parameter in module.parameters():\n",
    "        parameter.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f53b443f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.082255Z",
     "start_time": "2022-08-16T09:43:19.078081Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# 8-bit optimizer\n",
    "def set_embedding_parameters_bits(embeddings_path, optim_bits=32):\n",
    "    \"\"\"\n",
    "    https://github.com/huggingface/transformers/issues/14819#issuecomment-1003427930\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding_types = (\"word\", \"position\", \"token_type\")\n",
    "    for embedding_type in embedding_types:\n",
    "        attr_name = f\"{embedding_type}_embeddings\"\n",
    "        \n",
    "        if hasattr(embeddings_path, attr_name): \n",
    "            bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n",
    "                getattr(embeddings_path, attr_name), 'weight', {'optim_bits': optim_bits}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "893fcbbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.089773Z",
     "start_time": "2022-08-16T09:43:19.084256Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_loss_f1(model, criterion, loader, device):\n",
    "    \"\"\"損失・正解率を計算\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # デバイスの指定\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "            labels = data[\"labels\"].to(device)\n",
    "\n",
    "            # 順伝播\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            # 損失計算\n",
    "            loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # 確率計算\n",
    "            prob = torch.sigmoid(outputs)\n",
    "            pred = torch.where(prob > 0.5, 1, 0)\n",
    "\n",
    "            # f1スコア計算\n",
    "            f1 = f1_score(pred.cpu().numpy(), labels.cpu().numpy(), average=\"macro\", zero_division=0)\n",
    "\n",
    "    return loss / len(loader), f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f43bc5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.098005Z",
     "start_time": "2022-08-16T09:43:19.091288Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# EralyStopクラス\n",
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "        self,\n",
    "        patience,\n",
    "        threshold,\n",
    "        verbose=False,\n",
    "        path=\"/home/jovyan/work/data/checkpoint/checkpoint_model.pth\",\n",
    "    ):\n",
    "        \"\"\"引数：最小値の非更新数カウンタ、最小値判定の閾値, 表示設定、モデル格納path\"\"\"\n",
    "\n",
    "        self.patience = patience  # 設定ストップカウンタ\n",
    "        self.threshold = threshold  # 最小値判定の閾値。比率で指定\n",
    "        self.verbose = verbose  # 表示の有無\n",
    "        self.counter = 0  # 現在のカウンタ値\n",
    "        self.early_stop = False  # ストップフラグ\n",
    "        self.val_loss_min = np.Inf  # 前回のベストスコア記憶用\n",
    "        self.path = path  # ベストモデル格納path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss > (1 - self.threshold) * self.val_loss_min:  # ベストスコアを更新できなかった場合\n",
    "            self.counter += 1  # ストップカウンタを+1\n",
    "            if self.verbose:  # 表示を有効にした場合は経過を表示\n",
    "                print(\n",
    "                    f\"EarlyStopping counter: {self.counter} out of {self.patience}\"\n",
    "                )  # 現在のカウンタを表示する\n",
    "            if self.counter >= self.patience:  # 設定カウントを上回ったらストップフラグをTrueに変更\n",
    "                self.early_stop = True\n",
    "\n",
    "        else:  # ベストスコアを更新した場合\n",
    "            if self.verbose:  # 表示を有効にした場合は、前回のベストスコアからどれだけ更新したか？を表示\n",
    "                print(\n",
    "                    f\"Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...\"\n",
    "                )\n",
    "            torch.save(model.state_dict(), self.path)  # ベストモデルを指定したpathに保存\n",
    "\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0  # ストップカウンタリセット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf39f8",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "670e3dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.107927Z",
     "start_time": "2022-08-16T09:43:19.099543Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# BERT分類モデル\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            self.config[\"model_name\"], output_hidden_states=True\n",
    "        )\n",
    "        self.drop = nn.Dropout(self.config[\"drop_rate\"])\n",
    "        if self.config[\"header_type\"] == \"Linear\":\n",
    "            self.fc = nn.Linear(self.config[\"hidden_size\"], self.config[\"output_size\"])\n",
    "        elif self.config[\"header_type\"] == \"Pooling\":\n",
    "            self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "            self.fc = nn.Linear(\n",
    "                self.config[\"hidden_size\"], self.config[\"output_size\"]\n",
    "            )\n",
    "        elif self.config[\"header_type\"] == \"Couvolution\":\n",
    "            self.cnn1 = nn.Conv1d(\n",
    "                self.config[\"hidden_size\"], 256, kernel_size=2, padding=1\n",
    "            )\n",
    "            self.cnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)\n",
    "        elif self.config[\"header_type\"] == \"Concatenate\":\n",
    "            self.fc = nn.Linear(\n",
    "                self.config[\"hidden_size\"] * 4, self.config[\"output_size\"]\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Gradient Checkpointing\n",
    "        if self.config[\"gradient_checkpoint\"]:\n",
    "            self.bert.gradient_checkpointing_enable()\n",
    "        # Freeze\n",
    "        if self.config[\"freezing\"]:\n",
    "            freeze(self.bert.embeddings)\n",
    "            freeze(self.bert.encoder.layer[:2])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        x = self.bert(\n",
    "                input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids\n",
    "            )\n",
    "        if self.config[\"header_type\"] == \"Linear\":\n",
    "            x = self.drop(x.hidden_states[-1])\n",
    "            x = self.fc(x[:, 0, :])\n",
    "        elif self.config[\"header_type\"] == \"Pooling\":\n",
    "            x = self.drop(x.hidden_states[-1])\n",
    "            x, _ = x.max(1)\n",
    "            x = self.fc(x)\n",
    "        elif self.config[\"header_type\"] == \"Couvolution\":\n",
    "            x.hidden_states[-1].permute(0, 2, 1)\n",
    "            x = nn.functional.relu(self.cnn1(x))\n",
    "            x = self.cnn2(x)\n",
    "            x, _ = torch.max(x, 2)\n",
    "        elif self.config[\"header_type\"] == \"Concatenate\":\n",
    "            x = torch.cat(\n",
    "                [x[\"hidden_states\"][-1 * i][:, 0] for i in range(1, 4 + 1)], dim=1\n",
    "            )  # concatenate\n",
    "            x = self.fc(x)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019b8fb6",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3701a75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-16T09:43:19.119145Z",
     "start_time": "2022-08-16T09:43:19.109525Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    X_train_valid,\n",
    "    y_train_valid,\n",
    "    df_fold,\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    schedular,\n",
    "    config,\n",
    "    device=None,\n",
    "):\n",
    "    \"\"\"モデルの学習を実行し、損失・正解率のログを返す\"\"\"\n",
    "    # デバイスの指定\n",
    "    model.to(device)\n",
    "\n",
    "    # tokenizerの作成\n",
    "    _tokenizer = tokenizer(\n",
    "        AutoTokenizer.from_pretrained(config[\"model_name\"]), config[\"max_token_len\"]\n",
    "    )\n",
    "    # collate_fnの設定(dynamic padding)\n",
    "    # collate_fn = DataCollatorWithPadding(tokenizer=CFG.tokenizer)\n",
    "    collate_fn = Collate(_tokenizer)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # 学習\n",
    "    log_train = []\n",
    "    log_valid = []\n",
    "\n",
    "    earlystopping = EarlyStopping(patience=15, threshold=0.001, verbose=True)\n",
    "    for epoch_num in range(config[\"num_epochs\"]):\n",
    "        # 開始時刻の記録\n",
    "        s_time = time.time()\n",
    "\n",
    "        # Kfoldsの選択\n",
    "        fold = epoch_num % config[\"fold_split\"]\n",
    "        train_index =  df_fold[df_fold != fold].index\n",
    "        valid_index =  df_fold[df_fold == fold].index\n",
    "\n",
    "        # dataloaderの作成\n",
    "        dataset_train = CreateSubset(\n",
    "            X_train_valid, y_train_valid, _tokenizer, train_index\n",
    "        )\n",
    "        dataloader_train = DataLoader(\n",
    "            dataset_train,\n",
    "            batch_size=config[\"train_batch_size\"],\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        dataset_valid = CreateSubset(\n",
    "            X_train_valid, y_train_valid, _tokenizer, valid_index\n",
    "        )\n",
    "        dataloader_valid = DataLoader(\n",
    "            dataset_valid,\n",
    "            batch_size=config[\"valid_batch_size\"],\n",
    "            collate_fn=collate_fn,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "        # 訓練モードに設定\n",
    "        model.train()\n",
    "\n",
    "        # Automatic Mixed Precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        for data in tqdm(dataloader_train, leave=False):\n",
    "            # デバイスの指定\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "            labels = data[\"labels\"].to(device)\n",
    "\n",
    "            # 勾配をゼロで初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # 損失と正解率の算出\n",
    "            loss_train, f1_train = calculate_loss_f1(\n",
    "                model, criterion, dataloader_train, device\n",
    "            )\n",
    "            loss_valid, f1_valid = calculate_loss_f1(\n",
    "                model, criterion, dataloader_valid, device\n",
    "            )\n",
    "            log_train.append([loss_train, f1_train])\n",
    "            log_valid.append([loss_valid, f1_valid])\n",
    "\n",
    "            # 終了時刻の記録\n",
    "            e_time = time.time()\n",
    "\n",
    "            # 毎エポックearlystoppingの判定をさせる\n",
    "            if epoch_num > 0:\n",
    "                earlystopping(loss_valid, model)  # callメソッド呼び出し\n",
    "                if earlystopping.early_stop:  # ストップフラグがTrueの場合、breakでforループを抜ける\n",
    "                    print(\"Early Stopping!\")\n",
    "                    break\n",
    "        schedular.step()\n",
    "        # ログを出力\n",
    "        print(\n",
    "            f\"epoch: {epoch_num + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}, {(e_time - s_time):.4f}sec\"\n",
    "        )\n",
    "        if epoch_num > 0:\n",
    "            if earlystopping.early_stop:\n",
    "                break\n",
    "    return {\"train\": log_train, \"valid\": log_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93335120",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.101Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "  0%|          | 1/1226 [24:06<492:12:47, 1446.50s/it]"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = BERTClass(config)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=config[\"label_smoothing\"])\n",
    "\n",
    "# 8-bit optimizer\n",
    "model_parameters = filter(lambda parameter: parameter.requires_grad, model.parameters())\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model_parameters,\n",
    "    lr=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=config[\"lr_T_max\"], eta_min=config[\"min_lr\"]\n",
    ")\n",
    "# scheduler = create_lr_scheduler_with_warmup(\n",
    "#     torch_lr_scheduler,\n",
    "#     warmup_start_value=config[\"warmup_start_value\"],\n",
    "#     warmup_end_value=config[\"warmup_end_value\"],\n",
    "#     warmup_duration=config[\"warmup_duration\"],\n",
    "# )\n",
    "\n",
    "# デバイスの指定\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# モデルの学習\n",
    "log = train_model(\n",
    "    df_train_valid[\"inputs\"],\n",
    "    df_train_valid[dicourse_effectiveness_cols].to_numpy(),\n",
    "    df_train_valid[\"kfold\"],\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    config,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa58e3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.102Z"
    }
   },
   "outputs": [],
   "source": [
    "# ログの可視化\n",
    "x_axis = [x for x in range(1, len(log[\"train\"]) + 1)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax[0].plot(x_axis, np.array(log[\"train\"]).T[0], label=\"train\")\n",
    "ax[0].plot(x_axis, np.array(log[\"valid\"]).T[0], label=\"valid\")\n",
    "ax[0].set_xlabel(\"epoch\")\n",
    "ax[0].set_ylabel(\"loss\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(x_axis, np.array(log[\"train\"]).T[1], label=\"train\")\n",
    "ax[1].plot(x_axis, np.array(log[\"valid\"]).T[1], label=\"valid\")\n",
    "ax[1].set_xlabel(\"epoch\")\n",
    "ax[1].set_ylabel(\"accuracy\")\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a9a961",
   "metadata": {},
   "source": [
    "# infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac08ae",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.103Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Datasetの作成\n",
    "_tokenizer = tokenizer(AutoTokenizer.from_pretrained(config[\"model_name\"]), config[\"max_token_len\"])\n",
    "y_test = pd.get_dummies(df_test[\"discourse_effectiveness\"]).to_numpy()\n",
    "dataset_test = CreateDataset(df_test[\"discourse_text\"], y_test, _tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac88682",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.103Z"
    }
   },
   "outputs": [],
   "source": [
    "def prediction(model, dataset, device):\n",
    "    # Dataloaderの作成\n",
    "    loader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "  \n",
    "    prob = []\n",
    "    pred = []\n",
    "    label = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            # デバイスの指定\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device)\n",
    "            label.append(data[\"labels\"].to(device))\n",
    "\n",
    "            # 順伝播 + 予測値の取得 \n",
    "            outputs = model.forward(input_ids, attention_mask, token_type_ids)\n",
    "            prob.append(torch.sigmoid(outputs))\n",
    "            pred.append(torch.where(outputs>0.5, 1, 0))\n",
    "            \n",
    "        pred = torch.cat(pred, dim=0)\n",
    "        prob = torch.cat(prob, dim=0)\n",
    "        label = torch.cat(label, dim=0)\n",
    "        \n",
    "        del model, loader\n",
    "        gc.collect()\n",
    "        return pred, label, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7410a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.104Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred, y_label, y_prob = prediction(model, dataset_test, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d3dd7c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.104Z"
    }
   },
   "outputs": [],
   "source": [
    "#precision, recallの算出\n",
    "def calculate_pr_rc(pred, label):\n",
    "    pr_rc = []\n",
    "    for col in pred.columns:\n",
    "        pr = precision_score(pred.loc[:, col].values, label.loc[:, col].values)\n",
    "        rc = recall_score(pred.loc[:, col].values, label.loc[:, col].values)\n",
    "        pr_rc.append([pr, rc])\n",
    "    return pr_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023fbad",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.105Z"
    }
   },
   "outputs": [],
   "source": [
    "precision_score(y_pred.to('cpu').numpy(), y_label.to('cpu').numpy(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb742c7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.106Z"
    }
   },
   "outputs": [],
   "source": [
    "recall_score(y_pred.to('cpu').numpy(), y_label.to('cpu').numpy(), average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26faf96",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.107Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_cross_entropy(y_prob, y_label):\n",
    "    loss = -1 * np.mean(np.sum( y_label * np.log(y_prob), axis=1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00996878",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.107Z"
    }
   },
   "outputs": [],
   "source": [
    "calc_cross_entropy(y_prob.to('cpu').numpy(), y_label.to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13106d8a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.108Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/home/jovyan/work/data/checkpoint/checkpoint_model_.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c761f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.108Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_performance(performances):\n",
    "    plt.rcParams[\"font.size\"] = 16\n",
    "    \n",
    "    length = len(performances.columns)\n",
    "    \n",
    "    plt.figure(figsize=(25, 10))\n",
    "    left = np.arange(length)\n",
    "    \n",
    "    plt.bar(left - 0.2, performances.loc[\"precision\", :], width=0.4, label=\"precision\")\n",
    "    plt.bar(left + 0.2, performances.loc[\"recall\", :], width=0.4, label=\"recall\")\n",
    "    plt.xticks(np.arange(length), performances.columns.tolist(), rotation=90)\n",
    "    plt.yticks(np.arange(11)*0.1, np.round(np.arange(11)*0.1, 1))\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.title(\"precision  vs recall \")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f64868",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-16T09:43:13.109Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_performance(test_pr_rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e5992a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d44ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64df16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65c083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395dd5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
